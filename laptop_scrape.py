# -*- coding: utf-8 -*-
"""Laptop_Scrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yJa3y6AmHG2oNOjoFtt5ihYysSQO6LPY
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import pandas as pd
import time

if 'driver' in locals() and driver:
    try:
        driver.quit()
        print("Quit existing driver instance.")
    except Exception as e:
        print(f"Error quitting existing driver: {e}")

# Setup Chrome options for headless scraping
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 10)

# Start URL - static phones page, page=1
url = "https://webscraper.io/test-sites/e-commerce/static/phones/touch?page=1"
driver.get(url)

phones = []

while True:
    try:
        # Wait for products to load
        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.thumbnail")))
        products = driver.find_elements(By.CSS_SELECTOR, "div.thumbnail")

        if not products:
            print("No products found on the current page. Ending scrape.")
            break

        for product in products:
            try:
                title = product.find_element(By.CSS_SELECTOR, "a.title").text
            except NoSuchElementException:
                title = "N/A"

            try:
                description = product.find_element(By.CSS_SELECTOR, "p.description").text
            except NoSuchElementException:
                description = "N/A"

            try:
                price = product.find_element(By.CSS_SELECTOR, "h4.price").text
            except NoSuchElementException:
                price = "N/A"

            try:
                reviews = product.find_element(By.CSS_SELECTOR, "div.ratings p.pull-right").text
            except NoSuchElementException:
                reviews = "0 reviews"

            try:
                stars = product.find_elements(By.CSS_SELECTOR, "div.ratings span.glyphicon-star")
                rating = len(stars)
            except NoSuchElementException:
                rating = 0

            phones.append({
                "title": title,
                "description": description,
                "price": price,
                "reviews": reviews,
                "rating": rating
            })

        # Find Next button li by aria-label
        try:
            next_li = driver.find_element(By.CSS_SELECTOR, 'ul.pagination li.page-item[aria-label="Next Â»"]')
            # Check if Next is enabled (has <a>) or disabled (has <span>)
            try:
                next_link = next_li.find_element(By.TAG_NAME, 'a')
                next_url = next_link.get_attribute('href')
                print(f"Moving to next page: {next_url}")
                driver.get(next_url)
                time.sleep(3)
            except NoSuchElementException:
                print("Next button disabled. End of pagination.")
                break
        except NoSuchElementException:
            print("Next button not found. End of pagination.")
            break

    except (TimeoutException, NoSuchElementException) as e:
        print(f"Error during scraping: {e}. Ending scrape.")
        break

# Close driver
driver.quit()
print("Driver quit successfully.")

# Create DataFrame and save to CSV
df = pd.DataFrame(phones)
print(f"Total phones scraped: {len(df)}")
print(df.head())
df.to_csv("phones_static_pagination.csv", index=False)